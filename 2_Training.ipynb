{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning\n",
    "\n",
    "## 2. Training CNN-RNN\n",
    "\n",
    "---\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "### Task #1\n",
    "\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.[This paper](https://arxiv.org/pdf/1502.03044.pdf)\n",
    "\n",
    "Parameter values obtained from [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1240/414113 [00:00<01:08, 6016.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.94s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:02<00:00, 6595.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## Parameters.\n",
    "batch_size = 128          # batch size\n",
    "vocab_threshold = 2        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# Image Transformation.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# The total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Training\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/3236], Loss: 3.5261, Perplexity: 33.99190\n",
      "Epoch [1/3], Step [200/3236], Loss: 3.3629, Perplexity: 28.8727\n",
      "Epoch [1/3], Step [300/3236], Loss: 2.9794, Perplexity: 19.6752\n",
      "Epoch [1/3], Step [400/3236], Loss: 2.9081, Perplexity: 18.3227\n",
      "Epoch [1/3], Step [500/3236], Loss: 3.0288, Perplexity: 20.67239\n",
      "Epoch [1/3], Step [600/3236], Loss: 2.8467, Perplexity: 17.2305\n",
      "Epoch [1/3], Step [700/3236], Loss: 2.7719, Perplexity: 15.9885\n",
      "Epoch [1/3], Step [800/3236], Loss: 2.5119, Perplexity: 12.3279\n",
      "Epoch [1/3], Step [900/3236], Loss: 2.5148, Perplexity: 12.3637\n",
      "Epoch [1/3], Step [1000/3236], Loss: 2.4851, Perplexity: 12.0018\n",
      "Epoch [1/3], Step [1100/3236], Loss: 2.3511, Perplexity: 10.4972\n",
      "Epoch [1/3], Step [1200/3236], Loss: 2.4205, Perplexity: 11.2510\n",
      "Epoch [1/3], Step [1300/3236], Loss: 2.9363, Perplexity: 18.8464\n",
      "Epoch [1/3], Step [1400/3236], Loss: 2.7007, Perplexity: 14.8906\n",
      "Epoch [1/3], Step [1500/3236], Loss: 2.4640, Perplexity: 11.7515\n",
      "Epoch [1/3], Step [1600/3236], Loss: 2.3390, Perplexity: 10.3709\n",
      "Epoch [1/3], Step [1700/3236], Loss: 2.3184, Perplexity: 10.1595\n",
      "Epoch [1/3], Step [1800/3236], Loss: 2.2032, Perplexity: 9.05352\n",
      "Epoch [1/3], Step [1900/3236], Loss: 2.2598, Perplexity: 9.58148\n",
      "Epoch [1/3], Step [2000/3236], Loss: 2.9293, Perplexity: 18.7153\n",
      "Epoch [1/3], Step [2100/3236], Loss: 2.4231, Perplexity: 11.2812\n",
      "Epoch [1/3], Step [2200/3236], Loss: 2.1067, Perplexity: 8.22129\n",
      "Epoch [1/3], Step [2300/3236], Loss: 2.3344, Perplexity: 10.3231\n",
      "Epoch [1/3], Step [2400/3236], Loss: 2.4168, Perplexity: 11.2104\n",
      "Epoch [1/3], Step [2500/3236], Loss: 2.2118, Perplexity: 9.13241\n",
      "Epoch [1/3], Step [2600/3236], Loss: 2.2862, Perplexity: 9.83728\n",
      "Epoch [1/3], Step [2700/3236], Loss: 2.1930, Perplexity: 8.96236\n",
      "Epoch [1/3], Step [2800/3236], Loss: 2.1883, Perplexity: 8.92009\n",
      "Epoch [1/3], Step [2900/3236], Loss: 2.1317, Perplexity: 8.42898\n",
      "Epoch [1/3], Step [3000/3236], Loss: 2.2360, Perplexity: 9.35579\n",
      "Epoch [1/3], Step [3100/3236], Loss: 2.2154, Perplexity: 9.16536\n",
      "Epoch [1/3], Step [3200/3236], Loss: 2.1848, Perplexity: 8.88858\n",
      "Epoch [2/3], Step [100/3236], Loss: 2.3419, Perplexity: 10.40093\n",
      "Epoch [2/3], Step [200/3236], Loss: 2.3609, Perplexity: 10.6000\n",
      "Epoch [2/3], Step [300/3236], Loss: 2.2018, Perplexity: 9.04147\n",
      "Epoch [2/3], Step [400/3236], Loss: 3.1353, Perplexity: 22.9949\n",
      "Epoch [2/3], Step [500/3236], Loss: 2.1283, Perplexity: 8.40063\n",
      "Epoch [2/3], Step [600/3236], Loss: 2.1640, Perplexity: 8.70616\n",
      "Epoch [2/3], Step [700/3236], Loss: 2.0551, Perplexity: 7.80773\n",
      "Epoch [2/3], Step [800/3236], Loss: 2.0027, Perplexity: 7.40929\n",
      "Epoch [2/3], Step [900/3236], Loss: 2.3521, Perplexity: 10.5075\n",
      "Epoch [2/3], Step [1000/3236], Loss: 2.3558, Perplexity: 10.5464\n",
      "Epoch [2/3], Step [1100/3236], Loss: 2.0341, Perplexity: 7.64568\n",
      "Epoch [2/3], Step [1200/3236], Loss: 2.0296, Perplexity: 7.61089\n",
      "Epoch [2/3], Step [1300/3236], Loss: 1.9335, Perplexity: 6.91366\n",
      "Epoch [2/3], Step [1400/3236], Loss: 1.8773, Perplexity: 6.53598\n",
      "Epoch [2/3], Step [1600/3236], Loss: 2.0526, Perplexity: 7.78847\n",
      "Epoch [2/3], Step [1700/3236], Loss: 1.9914, Perplexity: 7.32583\n",
      "Epoch [2/3], Step [1800/3236], Loss: 1.9944, Perplexity: 7.34815\n",
      "Epoch [2/3], Step [1900/3236], Loss: 1.9339, Perplexity: 6.91679\n",
      "Epoch [2/3], Step [2000/3236], Loss: 2.1551, Perplexity: 8.62855\n",
      "Epoch [2/3], Step [2100/3236], Loss: 2.1948, Perplexity: 8.97791\n",
      "Epoch [2/3], Step [2200/3236], Loss: 1.8970, Perplexity: 6.66600\n",
      "Epoch [2/3], Step [2300/3236], Loss: 2.4750, Perplexity: 11.8820\n",
      "Epoch [2/3], Step [2400/3236], Loss: 2.0713, Perplexity: 7.93538\n",
      "Epoch [2/3], Step [2500/3236], Loss: 2.0585, Perplexity: 7.83447\n",
      "Epoch [2/3], Step [2600/3236], Loss: 1.9377, Perplexity: 6.94260\n",
      "Epoch [2/3], Step [2700/3236], Loss: 2.0547, Perplexity: 7.80450\n",
      "Epoch [2/3], Step [2800/3236], Loss: 2.0506, Perplexity: 7.77287\n",
      "Epoch [2/3], Step [2900/3236], Loss: 1.9508, Perplexity: 7.03409\n",
      "Epoch [2/3], Step [3000/3236], Loss: 1.9998, Perplexity: 7.38741\n",
      "Epoch [2/3], Step [3100/3236], Loss: 1.9788, Perplexity: 7.23412\n",
      "Epoch [2/3], Step [3200/3236], Loss: 2.0607, Perplexity: 7.85143\n",
      "Epoch [3/3], Step [100/3236], Loss: 1.8991, Perplexity: 6.680049\n",
      "Epoch [3/3], Step [200/3236], Loss: 1.9827, Perplexity: 7.26229\n",
      "Epoch [3/3], Step [300/3236], Loss: 2.0379, Perplexity: 7.67463\n",
      "Epoch [3/3], Step [400/3236], Loss: 1.8951, Perplexity: 6.65329\n",
      "Epoch [3/3], Step [500/3236], Loss: 3.5788, Perplexity: 35.8301\n",
      "Epoch [3/3], Step [600/3236], Loss: 2.0783, Perplexity: 7.99115\n",
      "Epoch [3/3], Step [700/3236], Loss: 1.9018, Perplexity: 6.69778\n",
      "Epoch [3/3], Step [800/3236], Loss: 1.9041, Perplexity: 6.71353\n",
      "Epoch [3/3], Step [900/3236], Loss: 2.1190, Perplexity: 8.32328\n",
      "Epoch [3/3], Step [1000/3236], Loss: 1.8534, Perplexity: 6.3818\n",
      "Epoch [3/3], Step [1100/3236], Loss: 2.0482, Perplexity: 7.75363\n",
      "Epoch [3/3], Step [1200/3236], Loss: 1.9842, Perplexity: 7.27359\n",
      "Epoch [3/3], Step [1300/3236], Loss: 1.8923, Perplexity: 6.63470\n",
      "Epoch [3/3], Step [1400/3236], Loss: 2.4260, Perplexity: 11.3135\n",
      "Epoch [3/3], Step [1500/3236], Loss: 2.6693, Perplexity: 14.4292\n",
      "Epoch [3/3], Step [1600/3236], Loss: 1.9691, Perplexity: 7.16410\n",
      "Epoch [3/3], Step [1700/3236], Loss: 1.9111, Perplexity: 6.76081\n",
      "Epoch [3/3], Step [1800/3236], Loss: 2.1438, Perplexity: 8.53222\n",
      "Epoch [3/3], Step [1900/3236], Loss: 1.9456, Perplexity: 6.99799\n",
      "Epoch [3/3], Step [2000/3236], Loss: 1.9553, Perplexity: 7.06594\n",
      "Epoch [3/3], Step [2100/3236], Loss: 1.8544, Perplexity: 6.388197\n",
      "Epoch [3/3], Step [2200/3236], Loss: 2.4285, Perplexity: 11.3415\n",
      "Epoch [3/3], Step [2300/3236], Loss: 1.9809, Perplexity: 7.24950\n",
      "Epoch [3/3], Step [2400/3236], Loss: 1.8937, Perplexity: 6.64396\n",
      "Epoch [3/3], Step [2500/3236], Loss: 1.9108, Perplexity: 6.75831\n",
      "Epoch [3/3], Step [2600/3236], Loss: 1.7901, Perplexity: 5.99039\n",
      "Epoch [3/3], Step [2700/3236], Loss: 2.4173, Perplexity: 11.2158\n",
      "Epoch [3/3], Step [2800/3236], Loss: 1.9113, Perplexity: 6.76199\n",
      "Epoch [3/3], Step [2900/3236], Loss: 1.8974, Perplexity: 6.66877\n",
      "Epoch [3/3], Step [3000/3236], Loss: 1.8679, Perplexity: 6.47468\n",
      "Epoch [3/3], Step [3100/3236], Loss: 1.8286, Perplexity: 6.22530\n",
      "Epoch [3/3], Step [3200/3236], Loss: 2.0773, Perplexity: 7.98301\n",
      "Epoch [3/3], Step [3236/3236], Loss: 1.7941, Perplexity: 6.01414"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
